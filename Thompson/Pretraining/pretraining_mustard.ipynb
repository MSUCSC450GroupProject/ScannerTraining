{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce GTX 1660 SUPER, compute capability 7.5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import keras_nlp\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "\n",
    "policy = keras.mixed_precision.Policy(\"mixed_float16\")\n",
    "keras.mixed_precision.set_global_policy(policy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing params.\n",
    "PRETRAINING_BATCH_SIZE = 128 # was 128\n",
    "FINETUNING_BATCH_SIZE = 32\n",
    "SEQ_LENGTH = 128\n",
    "MASK_RATE = 0.25\n",
    "PREDICTIONS_PER_SEQ = 32\n",
    "\n",
    "# Model params.\n",
    "NUM_LAYERS = 3\n",
    "MODEL_DIM = 256\n",
    "INTERMEDIATE_DIM = 512\n",
    "NUM_HEADS = 4\n",
    "DROPOUT = 0.1\n",
    "NORM_EPSILON = 1e-5\n",
    "\n",
    "# Training params.\n",
    "PRETRAINING_LEARNING_RATE = 5e-4\n",
    "PRETRAINING_EPOCHS = 8\n",
    "FINETUNING_LEARNING_RATE = 5e-5\n",
    "FINETUNING_EPOCHS = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import sitcom data\n",
    "data_sitcoms = pd.read_csv('../../shared_data/mustard++_text.csv')\n",
    "\n",
    "# Adjust sitcom data\n",
    "data_sitcoms = data_sitcoms.drop(columns=['SCENE','KEY','END_TIME','SPEAKER','SHOW','Sarcasm_Type','Implicit_Emotion','Explicit_Emotion','Valence','Arousal'], axis=1)\n",
    "data_sitcoms = data_sitcoms.rename(columns={'SENTENCE':'text','Sarcasm':'label'})\n",
    "\n",
    "# remove empty label rows\n",
    "for index, row in data_sitcoms.iterrows():\n",
    "    if math.isnan(row['label']):\n",
    "        data_sitcoms = data_sitcoms.drop(index, axis='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_size = len(data_sitcoms.index) ## was static at 1400, now is dyanmic about entire subset\n",
    "testing_size = int(subset_size * 0.4) ## 40% testing size\n",
    "validation_size = int(subset_size * 0.2) ## 20% validation size\n",
    "shuffle_size = subset_size - validation_size ## Shuffle size for shuffling training batch\n",
    "\n",
    "data_batch_size = 6 ## was32\n",
    "\n",
    "data = data_sitcoms.sample(frac=1).reset_index(drop=True) ## was just data\n",
    "train_data = data.head(subset_size) ## was just data\n",
    "test_data = data.tail(testing_size) ## was just data\n",
    "\n",
    "## Make a train dataset\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (\n",
    "        train_data['text'][validation_size:], \n",
    "        train_data['label'][validation_size:]\n",
    "    )\n",
    ").shuffle(shuffle_size).batch(PRETRAINING_BATCH_SIZE) # was data_batch_size\n",
    "\n",
    "## Make a validation dataset\n",
    "val_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (\n",
    "        train_data['text'][:validation_size],\n",
    "        train_data['label'][:validation_size]\n",
    "    )\n",
    ").batch(PRETRAINING_BATCH_SIZE)\n",
    "\n",
    "## Make a test dataset\n",
    "test_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (\n",
    "        test_data['text'],\n",
    "        test_data['label']\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(4,), dtype=string, numpy=\n",
      "array([b'I did not mean to touch that,I mean',\n",
      "       b\"Are you sure she wasn't trying to breathe life back into him?\",\n",
      "       b\"Well, for me, too. It's not the same with you gone.\",\n",
      "       b'No you are not Chandler, we still love you Chandler'],\n",
      "      dtype=object)>, <tf.Tensor: shape=(4,), dtype=float64, numpy=array([0., 1., 0., 1.])>)\n"
     ]
    }
   ],
   "source": [
    "print(train_ds.unbatch().batch(4).take(1).get_single_element())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This layer will turn our input sentence into a list of 1s and 0s the same size\n",
    "# our vocabulary, indicating whether a word is present in absent.\n",
    "multi_hot_layer = keras.layers.TextVectorization(\n",
    "    max_tokens=4000, output_mode=\"multi_hot\"\n",
    ")\n",
    "multi_hot_layer.adapt(train_ds.map(lambda x, y: x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We then learn a linear regression over that layer, and that's our entire\n",
    "# baseline model!\n",
    "regression_layer = keras.layers.Dense(1, activation=\"sigmoid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "8/8 [==============================] - 1s 34ms/step - loss: 0.6954 - accuracy: 0.5052 - val_loss: 0.6921 - val_accuracy: 0.5125\n",
      "Epoch 2/5\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.6873 - accuracy: 0.5603 - val_loss: 0.6902 - val_accuracy: 0.5458\n",
      "Epoch 3/5\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.6814 - accuracy: 0.6050 - val_loss: 0.6887 - val_accuracy: 0.5625\n",
      "Epoch 4/5\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.6764 - accuracy: 0.6403 - val_loss: 0.6885 - val_accuracy: 0.5708\n",
      "Epoch 5/5\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.6714 - accuracy: 0.6736 - val_loss: 0.6873 - val_accuracy: 0.5917\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x16c15ff3d30>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(), dtype=\"string\")\n",
    "outputs = regression_layer(multi_hot_layer(inputs))\n",
    "baseline_model = keras.Model(inputs, outputs)\n",
    "baseline_model.compile(loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "baseline_model.fit(train_ds, validation_data=val_ds, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download pretraining data.\n",
    "keras.utils.get_file(\n",
    "    origin=\"https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-raw-v1.zip\",\n",
    "    extract=True,\n",
    ")\n",
    "wiki_dir = os.path.expanduser(\"~/.keras/datasets/wikitext-103-raw/\")\n",
    "\n",
    "# Download finetuning data.\n",
    "keras.utils.get_file(\n",
    "    origin=\"https://dl.fbaipublicfiles.com/glue/data/SST-2.zip\", extract=True,\n",
    ")\n",
    "sst_dir = os.path.expanduser(\"~/.keras/datasets/SST-2/\")\n",
    "\n",
    "# Download vocabulary data.\n",
    "vocab_file = keras.utils.get_file(\n",
    "    origin=\"https://storage.googleapis.com/tensorflow/keras-nlp/examples/bert/bert_vocab_uncased.txt\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(4,), dtype=string, numpy=\n",
      "array([b'hide new secretions from the parental units ',\n",
      "       b'contains no wit , only labored gags ',\n",
      "       b'that loves its characters and communicates something rather beautiful about human nature ',\n",
      "       b'remains utterly satisfied to remain the same throughout '],\n",
      "      dtype=object)>, <tf.Tensor: shape=(4,), dtype=int32, numpy=array([0, 0, 1, 0])>)\n"
     ]
    }
   ],
   "source": [
    "# Load SST-2.\n",
    "sst_train_ds = tf.data.experimental.CsvDataset(\n",
    "    sst_dir + \"train.tsv\", [tf.string, tf.int32], header=True, field_delim=\"\\t\"\n",
    ").batch(FINETUNING_BATCH_SIZE)\n",
    "sst_val_ds = tf.data.experimental.CsvDataset(\n",
    "    sst_dir + \"dev.tsv\", [tf.string, tf.int32], header=True, field_delim=\"\\t\"\n",
    ").batch(FINETUNING_BATCH_SIZE)\n",
    "\n",
    "# Load wikitext-103 and filter out short lines.\n",
    "wiki_train_ds = (\n",
    "    tf.data.TextLineDataset(wiki_dir + \"wiki.train.raw\")\n",
    "    .filter(lambda x: tf.strings.length(x) > 100)\n",
    "    .batch(PRETRAINING_BATCH_SIZE)\n",
    ")\n",
    "wiki_val_ds = (\n",
    "    tf.data.TextLineDataset(wiki_dir + \"wiki.valid.raw\")\n",
    "    .filter(lambda x: tf.strings.length(x) > 100)\n",
    "    .batch(PRETRAINING_BATCH_SIZE)\n",
    ")\n",
    "\n",
    "# Take a peak at the sst-2 dataset.\n",
    "print(sst_train_ds.unbatch().batch(4).take(1).get_single_element())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "2105/2105 [==============================] - 14s 6ms/step - loss: 0.6445 - accuracy: 0.6353 - val_loss: 0.5880 - val_accuracy: 0.7018\n",
      "Epoch 2/5\n",
      "2105/2105 [==============================] - 11s 5ms/step - loss: 0.5982 - accuracy: 0.6812 - val_loss: 0.5601 - val_accuracy: 0.7225\n",
      "Epoch 3/5\n",
      "2105/2105 [==============================] - 11s 5ms/step - loss: 0.5766 - accuracy: 0.6939 - val_loss: 0.5514 - val_accuracy: 0.7317\n",
      "Epoch 4/5\n",
      "2105/2105 [==============================] - 11s 5ms/step - loss: 0.5641 - accuracy: 0.7013 - val_loss: 0.5501 - val_accuracy: 0.7339\n",
      "Epoch 5/5\n",
      "2105/2105 [==============================] - 12s 6ms/step - loss: 0.5561 - accuracy: 0.7057 - val_loss: 0.5521 - val_accuracy: 0.7351\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x16c753bee50>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This layer will turn our input sentence into a list of 1s and 0s the same size\n",
    "# our vocabulary, indicating whether a word is present in absent.\n",
    "multi_hot_layer = keras.layers.TextVectorization(\n",
    "    max_tokens=4000, output_mode=\"multi_hot\"\n",
    ")\n",
    "multi_hot_layer.adapt(train_ds.map(lambda x, y: x)) #was sst_train_ds\n",
    "# We then learn a linear regression over that layer, and that's our entire\n",
    "# baseline model!\n",
    "regression_layer = keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "\n",
    "inputs = keras.Input(shape=(), dtype=\"string\")\n",
    "outputs = regression_layer(multi_hot_layer(inputs))\n",
    "baseline_model = keras.Model(inputs, outputs)\n",
    "baseline_model.compile(loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "baseline_model.fit(sst_train_ds, validation_data=sst_val_ds, epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'tokens': <tf.Tensor: shape=(128, 64), dtype=int32, numpy=\n",
      "array([[  103,   103,  2271, ...,  1999,  1007,  1998],\n",
      "       [  103,  7849,  5138, ..., 17042,  1014,  1030],\n",
      "       [ 1996,   103,  3940, ..., 13311,   103,  8336],\n",
      "       ...,\n",
      "       [ 2076,  1996,  2307, ...,   103,  1010,  1996],\n",
      "       [  103,   103,  2083, ...,     0,     0,     0],\n",
      "       [  103,  2007,  1045, ...,     0,     0,     0]])>, 'mask_positions': <tf.Tensor: shape=(128, 32), dtype=int64, numpy=\n",
      "array([[0, 1, 3, ..., 0, 0, 0],\n",
      "       [0, 2, 5, ..., 0, 0, 0],\n",
      "       [1, 2, 8, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [2, 8, 9, ..., 0, 0, 0],\n",
      "       [0, 1, 3, ..., 0, 0, 0],\n",
      "       [0, 3, 8, ..., 0, 0, 0]], dtype=int64)>}, <tf.Tensor: shape=(128, 32), dtype=int32, numpy=\n",
      "array([[ 7570,  7849, 13091, ...,     0,     0,     0],\n",
      "       [ 7570,  2271,  2003, ...,     0,     0,     0],\n",
      "       [ 2034,  3940,  4273, ...,     0,     0,     0],\n",
      "       ...,\n",
      "       [ 2307,  2103, 23133, ...,     0,     0,     0],\n",
      "       [ 3216,  2225,  4027, ...,     0,     0,     0],\n",
      "       [ 9794,  1030,  1996, ...,     0,     0,     0]])>, <tf.Tensor: shape=(128, 32), dtype=float16, numpy=\n",
      "array([[1., 1., 1., ..., 0., 0., 0.],\n",
      "       [1., 1., 1., ..., 0., 0., 0.],\n",
      "       [1., 1., 1., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [1., 1., 1., ..., 0., 0., 0.],\n",
      "       [1., 1., 1., ..., 0., 0., 0.],\n",
      "       [1., 1., 1., ..., 0., 0., 0.]], dtype=float16)>)\n"
     ]
    }
   ],
   "source": [
    "# Setting sequence_length will trim or pad the token outputs to shape\n",
    "# (batch_size, SEQ_LENGTH).\n",
    "tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(\n",
    "    vocabulary=vocab_file,\n",
    "    sequence_length=SEQ_LENGTH,\n",
    "    lowercase=True,\n",
    "    strip_accents=True,\n",
    ")\n",
    "# Setting mask_selection_length will trim or pad the mask outputs to shape\n",
    "# (batch_size, PREDICTIONS_PER_SEQ).\n",
    "masker = keras_nlp.layers.MLMMaskGenerator(\n",
    "    vocabulary_size=tokenizer.vocabulary_size(),\n",
    "    mask_selection_rate=MASK_RATE,\n",
    "    mask_selection_length=PREDICTIONS_PER_SEQ,\n",
    "    mask_token_id=tokenizer.token_to_id(\"[MASK]\"),\n",
    ")\n",
    "\n",
    "\n",
    "def preprocess(inputs):\n",
    "    inputs = tokenizer(inputs)\n",
    "    outputs = masker(inputs)\n",
    "    # Split the masking layer outputs into a (features, labels, and weights)\n",
    "    # tuple that we can use with keras.Model.fit().\n",
    "    features = {\n",
    "        \"tokens\": outputs[\"tokens\"],\n",
    "        \"mask_positions\": outputs[\"mask_positions\"],\n",
    "    }\n",
    "    labels = outputs[\"mask_ids\"]\n",
    "    weights = outputs[\"mask_weights\"]\n",
    "    return features, labels, weights\n",
    "\n",
    "\n",
    "# We use prefetch() to pre-compute preprocessed batches on the fly on the CPU.\n",
    "pretrain_ds = wiki_train_ds.map(\n",
    "    preprocess, num_parallel_calls=tf.data.AUTOTUNE\n",
    ").prefetch(tf.data.AUTOTUNE)\n",
    "pretrain_val_ds = wiki_val_ds.map(\n",
    "    preprocess, num_parallel_calls=tf.data.AUTOTUNE\n",
    ").prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Preview a single input example.\n",
    "# The masks will change each time you run the cell.\n",
    "print(pretrain_val_ds.take(1).get_single_element())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\natth\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\initializers\\initializers_v2.py:120: UserWarning: The initializer GlorotUniform is unseeded and being called multiple times, which will return identical values  each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 64)]              0         \n",
      "                                                                 \n",
      " token_and_position_embeddin  (None, 64, 256)          7830016   \n",
      " g (TokenAndPositionEmbeddin                                     \n",
      " g)                                                              \n",
      "                                                                 \n",
      " layer_normalization (LayerN  (None, 64, 256)          512       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64, 256)           0         \n",
      "                                                                 \n",
      " transformer_encoder (Transf  (None, 64, 256)          527104    \n",
      " ormerEncoder)                                                   \n",
      "                                                                 \n",
      " transformer_encoder_1 (Tran  (None, 64, 256)          527104    \n",
      " sformerEncoder)                                                 \n",
      "                                                                 \n",
      " transformer_encoder_2 (Tran  (None, 64, 256)          527104    \n",
      " sformerEncoder)                                                 \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9,411,840\n",
      "Trainable params: 9,411,840\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(SEQ_LENGTH,), dtype=tf.int32)\n",
    "\n",
    "# Embed our tokens with a positional embedding.\n",
    "embedding_layer = keras_nlp.layers.TokenAndPositionEmbedding(\n",
    "    vocabulary_size=tokenizer.vocabulary_size(),\n",
    "    sequence_length=SEQ_LENGTH,\n",
    "    embedding_dim=MODEL_DIM,\n",
    ")\n",
    "outputs = embedding_layer(inputs)\n",
    "\n",
    "# Apply layer normalization and dropout to the embedding.\n",
    "outputs = keras.layers.LayerNormalization(epsilon=NORM_EPSILON)(outputs)\n",
    "outputs = keras.layers.Dropout(rate=DROPOUT)(outputs)\n",
    "\n",
    "# Add a number of encoder blocks\n",
    "for i in range(NUM_LAYERS):\n",
    "    outputs = keras_nlp.layers.TransformerEncoder(\n",
    "        intermediate_dim=INTERMEDIATE_DIM,\n",
    "        num_heads=NUM_HEADS,\n",
    "        dropout=DROPOUT,\n",
    "        layer_norm_epsilon=NORM_EPSILON,\n",
    "    )(outputs)\n",
    "\n",
    "encoder_model = keras.Model(inputs, outputs)\n",
    "encoder_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Graph execution error:\n\nDetected at node 'StatefulPartitionedCall' defined at (most recent call last):\n    File \"c:\\Users\\natth\\Anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"c:\\Users\\natth\\Anaconda3\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"c:\\Users\\natth\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"c:\\Users\\natth\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n      app.start()\n    File \"c:\\Users\\natth\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 677, in start\n      self.io_loop.start()\n    File \"c:\\Users\\natth\\Anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"c:\\Users\\natth\\Anaconda3\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"c:\\Users\\natth\\Anaconda3\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"c:\\Users\\natth\\Anaconda3\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"c:\\Users\\natth\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 471, in dispatch_queue\n      await self.process_one()\n    File \"c:\\Users\\natth\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 460, in process_one\n      await dispatch(*args)\n    File \"c:\\Users\\natth\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 367, in dispatch_shell\n      await result\n    File \"c:\\Users\\natth\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 662, in execute_request\n      reply_content = await reply_content\n    File \"c:\\Users\\natth\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 360, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"c:\\Users\\natth\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 532, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"c:\\Users\\natth\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2863, in run_cell\n      result = self._run_cell(\n    File \"c:\\Users\\natth\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2909, in _run_cell\n      return runner(coro)\n    File \"c:\\Users\\natth\\Anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"c:\\Users\\natth\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3106, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"c:\\Users\\natth\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3309, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"c:\\Users\\natth\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3369, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\natth\\AppData\\Local\\Temp\\ipykernel_19224\\1795301261.py\", line 27, in <cell line: 27>\n      pretraining_model.fit(\n    File \"C:\\Users\\natth\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\natth\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"C:\\Users\\natth\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"C:\\Users\\natth\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\nNode: 'StatefulPartitionedCall'\nOut of memory while trying to allocate 2017460224 bytes.\n\t [[{{node StatefulPartitionedCall}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_58798]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\natth\\Desktop\\School Work\\Github School Code\\ScannerTraining\\Thompson\\Pretraining\\pretraining_mustard.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 27>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/natth/Desktop/School%20Work/Github%20School%20Code/ScannerTraining/Thompson/Pretraining/pretraining_mustard.ipynb#X16sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m pretraining_model\u001b[39m.\u001b[39mcompile(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/natth/Desktop/School%20Work/Github%20School%20Code/ScannerTraining/Thompson/Pretraining/pretraining_mustard.ipynb#X16sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     loss\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msparse_categorical_crossentropy\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/natth/Desktop/School%20Work/Github%20School%20Code/ScannerTraining/Thompson/Pretraining/pretraining_mustard.ipynb#X16sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     optimizer\u001b[39m=\u001b[39mkeras\u001b[39m.\u001b[39moptimizers\u001b[39m.\u001b[39mAdam(learning_rate\u001b[39m=\u001b[39mPRETRAINING_LEARNING_RATE),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/natth/Desktop/School%20Work/Github%20School%20Code/ScannerTraining/Thompson/Pretraining/pretraining_mustard.ipynb#X16sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     weighted_metrics\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39msparse_categorical_accuracy\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/natth/Desktop/School%20Work/Github%20School%20Code/ScannerTraining/Thompson/Pretraining/pretraining_mustard.ipynb#X16sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     jit_compile\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/natth/Desktop/School%20Work/Github%20School%20Code/ScannerTraining/Thompson/Pretraining/pretraining_mustard.ipynb#X16sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/natth/Desktop/School%20Work/Github%20School%20Code/ScannerTraining/Thompson/Pretraining/pretraining_mustard.ipynb#X16sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39m# Pretrain the model on our wiki text dataset.\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/natth/Desktop/School%20Work/Github%20School%20Code/ScannerTraining/Thompson/Pretraining/pretraining_mustard.ipynb#X16sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m pretraining_model\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/natth/Desktop/School%20Work/Github%20School%20Code/ScannerTraining/Thompson/Pretraining/pretraining_mustard.ipynb#X16sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     pretrain_ds, validation_data\u001b[39m=\u001b[39;49mpretrain_val_ds, epochs\u001b[39m=\u001b[39;49mPRETRAINING_EPOCHS,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/natth/Desktop/School%20Work/Github%20School%20Code/ScannerTraining/Thompson/Pretraining/pretraining_mustard.ipynb#X16sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/natth/Desktop/School%20Work/Github%20School%20Code/ScannerTraining/Thompson/Pretraining/pretraining_mustard.ipynb#X16sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39m# Save this base model for further finetuning.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/natth/Desktop/School%20Work/Github%20School%20Code/ScannerTraining/Thompson/Pretraining/pretraining_mustard.ipynb#X16sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m encoder_model\u001b[39m.\u001b[39msave(\u001b[39m\"\u001b[39m\u001b[39mencoder_model\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: Graph execution error:\n\nDetected at node 'StatefulPartitionedCall' defined at (most recent call last):\n    File \"c:\\Users\\natth\\Anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"c:\\Users\\natth\\Anaconda3\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"c:\\Users\\natth\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"c:\\Users\\natth\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n      app.start()\n    File \"c:\\Users\\natth\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 677, in start\n      self.io_loop.start()\n    File \"c:\\Users\\natth\\Anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"c:\\Users\\natth\\Anaconda3\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"c:\\Users\\natth\\Anaconda3\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"c:\\Users\\natth\\Anaconda3\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"c:\\Users\\natth\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 471, in dispatch_queue\n      await self.process_one()\n    File \"c:\\Users\\natth\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 460, in process_one\n      await dispatch(*args)\n    File \"c:\\Users\\natth\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 367, in dispatch_shell\n      await result\n    File \"c:\\Users\\natth\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 662, in execute_request\n      reply_content = await reply_content\n    File \"c:\\Users\\natth\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 360, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"c:\\Users\\natth\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 532, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"c:\\Users\\natth\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2863, in run_cell\n      result = self._run_cell(\n    File \"c:\\Users\\natth\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2909, in _run_cell\n      return runner(coro)\n    File \"c:\\Users\\natth\\Anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"c:\\Users\\natth\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3106, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"c:\\Users\\natth\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3309, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"c:\\Users\\natth\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3369, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\natth\\AppData\\Local\\Temp\\ipykernel_19224\\1795301261.py\", line 27, in <cell line: 27>\n      pretraining_model.fit(\n    File \"C:\\Users\\natth\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\natth\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"C:\\Users\\natth\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"C:\\Users\\natth\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\nNode: 'StatefulPartitionedCall'\nOut of memory while trying to allocate 2017460224 bytes.\n\t [[{{node StatefulPartitionedCall}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_58798]"
     ]
    }
   ],
   "source": [
    "# Create the pretraining model by attaching a masked language model head.\n",
    "inputs = {\n",
    "    \"tokens\": keras.Input(shape=(SEQ_LENGTH,), dtype=tf.int32),\n",
    "    \"mask_positions\": keras.Input(shape=(PREDICTIONS_PER_SEQ,), dtype=tf.int32),\n",
    "}\n",
    "\n",
    "# Encode the tokens.\n",
    "encoded_tokens = encoder_model(inputs[\"tokens\"])\n",
    "\n",
    "# Predict an output word for each masked input token.\n",
    "# We use the input token embedding to project from our encoded vectors to\n",
    "# vocabulary logits, which has been shown to improve training efficiency.\n",
    "outputs = keras_nlp.layers.MLMHead(\n",
    "    embedding_weights=embedding_layer.token_embedding.embeddings, activation=\"softmax\",\n",
    ")(encoded_tokens, mask_positions=inputs[\"mask_positions\"])\n",
    "\n",
    "# Define and compile our pretraining model.\n",
    "pretraining_model = keras.Model(inputs, outputs)\n",
    "pretraining_model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=PRETRAINING_LEARNING_RATE),\n",
    "    weighted_metrics=[\"sparse_categorical_accuracy\"],\n",
    "    jit_compile=True,\n",
    ")\n",
    "\n",
    "# Pretrain the model on our wiki text dataset.\n",
    "pretraining_model.fit(\n",
    "    pretrain_ds, validation_data=pretrain_val_ds, epochs=PRETRAINING_EPOCHS,\n",
    ")\n",
    "\n",
    "# Save this base model for further finetuning.\n",
    "encoder_model.save(\"encoder_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sentences, labels):\n",
    "    return tokenizer(sentences), labels\n",
    "\n",
    "\n",
    "# We use prefetch() to pre-compute preprocessed batches on the fly on our CPU.\n",
    "finetune_ds = sst_train_ds.map(\n",
    "    preprocess, num_parallel_calls=tf.data.AUTOTUNE\n",
    ").prefetch(tf.data.AUTOTUNE)\n",
    "finetune_val_ds = sst_val_ds.map(\n",
    "    preprocess, num_parallel_calls=tf.data.AUTOTUNE\n",
    ").prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Preview a single input example.\n",
    "print(finetune_val_ds.take(1).get_single_element())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the encoder model from disk so we can restart fine-tuning from scratch.\n",
    "encoder_model = keras.models.load_model(\"encoder_model\", compile=False)\n",
    "\n",
    "# Take as input the tokenized input.\n",
    "inputs = keras.Input(shape=(SEQ_LENGTH,), dtype=tf.int32)\n",
    "\n",
    "# Encode and pool the tokens.\n",
    "encoded_tokens = encoder_model(inputs)\n",
    "pooled_tokens = keras.layers.GlobalAveragePooling1D()(encoded_tokens)\n",
    "\n",
    "# Predict an output label.\n",
    "outputs = keras.layers.Dense(1, activation=\"sigmoid\")(pooled_tokens)\n",
    "\n",
    "# Define and compile our finetuning model.\n",
    "finetuning_model = keras.Model(inputs, outputs)\n",
    "finetuning_model.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=FINETUNING_LEARNING_RATE),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Finetune the model for the SST-2 task.\n",
    "finetuning_model.fit(\n",
    "    finetune_ds, validation_data=finetune_val_ds, epochs=FINETUNING_EPOCHS,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add our tokenization into our final model.\n",
    "inputs = keras.Input(shape=(), dtype=tf.string)\n",
    "tokens = tokenizer(inputs)\n",
    "outputs = finetuning_model(tokens)\n",
    "final_model = keras.Model(inputs, outputs)\n",
    "final_model.save(\"final_model\")\n",
    "\n",
    "# This model can predict directly on raw text.\n",
    "restored_model = keras.models.load_model(\"final_model\", compile=False)\n",
    "inference_data = tf.constant([\"Terrible, no good, trash.\", \"So great; I loved it!\"])\n",
    "print(restored_model(inference_data))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dd53c7222885d7750057d794778faebdb531c8e31dbaca85af85a4aaf6256611"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
