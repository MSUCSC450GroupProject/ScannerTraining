{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.run_functions_eagerly(True)\n",
    "tf.data.experimental.enable_debug_mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_news_headlines = pd.read_json(\"../shared_data/x1.json\")\n",
    "\n",
    "# Adjust news headline data\n",
    "data_news_headlines = data_news_headlines.drop(columns='article_link', axis=1)\n",
    "data_news_headlines = data_news_headlines.rename(columns ={'headline':'text', 'is_sarcastic':'label'})\n",
    "data_news_headlines = data_news_headlines.reindex(columns=['text','label'])\n",
    "data_news_headlines.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tweets = pd.read_csv(\"../shared_data/dataset_csv.csv\")\n",
    "\n",
    "# Adjust tweets data\n",
    "data_tweets = data_tweets.rename(columns={'tweets':'text'})\n",
    "data_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sitcoms = pd.read_csv(\"../shared_data/mustard++_text.csv\")\n",
    "\n",
    "# Adjust sitcom data\n",
    "data_sitcoms = data_sitcoms.drop(columns=['SCENE','KEY','END_TIME','SPEAKER','SHOW','Sarcasm_Type','Implicit_Emotion','Explicit_Emotion','Valence','Arousal'], axis=1)\n",
    "data_sitcoms = data_sitcoms.rename(columns={'SENTENCE':'text','Sarcasm':'label'})\n",
    "\n",
    "# remove empty label rows\n",
    "for index, row in data_sitcoms.iterrows():\n",
    "    if math.isnan(row['label']):\n",
    "        data_sitcoms = data_sitcoms.drop(index, axis='index')\n",
    "\n",
    "data_sitcoms.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_reddit = pd.read_csv(\"../shared_data/train-balanced-sarcasm.csv\")\n",
    "\n",
    "# Adjust reddit data\n",
    "data_reddit = data_reddit.drop(columns=['author','subreddit','score','ups','downs','date','created_utc','parent_comment'], axis=1)\n",
    "data_reddit = data_reddit.rename(columns={'comment':'text'})\n",
    "data_reddit = data_reddit.reindex(columns=['text','label'])\n",
    "\n",
    "data_reddit.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set News Headlines dataset variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_size = len(data_news_headlines.index)\n",
    "testing_size = int(subset_size * 0.2)\n",
    "validation_size = testing_size\n",
    "shuffle_size = subset_size - validation_size\n",
    "\n",
    "data_batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffle the data and set the train and test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_news_headlines.sample(frac=1).reset_index(drop=True)\n",
    "train_data = data.head(subset_size - testing_size)\n",
    "test_data = data.tail(testing_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (\n",
    "        train_data['text'][validation_size:], \n",
    "        train_data['label'][validation_size:]\n",
    "    )\n",
    ").shuffle(shuffle_size).batch(data_batch_size)\n",
    "\n",
    "val_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (\n",
    "        train_data['text'][:validation_size],\n",
    "        train_data['label'][:validation_size]\n",
    "    )\n",
    ").batch(data_batch_size)\n",
    "\n",
    "test_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (\n",
    "        test_data['text'],\n",
    "        test_data['label']\n",
    "    )\n",
    ")\n",
    "\n",
    "text_vocab_ds = tf.data.Dataset.from_tensor_slices(train_data['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set training variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 400\n",
    "\n",
    "##define the parameters for tokenizing and padding\n",
    "vocab_size = 10000\n",
    "embedding_dim = 32\n",
    "max_length = 120"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize and config the Weights and Biases graphing library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"sarcasmscanner\", entity=\"awesomepossum\")\n",
    "\n",
    "wandb.config = {\n",
    "    \"epochs\": epochs,\n",
    "    \"vocab_size\": vocab_size,\n",
    "    \"embedding_dim\": embedding_dim,\n",
    "    \"max_sentence_word_length\": max_length,\n",
    "    \"batch_size\": data_batch_size,\n",
    "    \"subset_size\": subset_size,\n",
    "    \"training_size\": subset_size - testing_size - validation_size,\n",
    "    \"testing_size\": testing_size,\n",
    "    \"validation_size\": validation_size,\n",
    "    \"dataset\": \"news_headlines\",\n",
    "    \"architecture\": \"LSTM\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the text vectorization layer and create the vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_layer = tf.keras.layers.TextVectorization(max_tokens=vocab_size, standardize='lower_and_strip_punctuation', split='whitespace', output_mode='int', output_sequence_length=max_length)\n",
    "\n",
    "vectorize_layer.adapt(text_vocab_ds.batch(data_batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    vectorize_layer,\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x=train_ds,\n",
    "                        validation_data=val_ds,\n",
    "                        epochs=epochs,\n",
    "                        callbacks=[WandbCallback()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(test_ds.batch(32))\n",
    "\n",
    "print(f'Loss: {loss}')\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the accuracy and loss over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(history, metric):\n",
    "  plt.plot(history.history[metric])\n",
    "  plt.plot(history.history['val_'+metric])\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(metric)\n",
    "  plt.legend([metric, 'val_'+metric])\n",
    "  plt.show()\n",
    "  \n",
    "plot_metrics(history, \"accuracy\")\n",
    "plot_metrics(history, \"loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_path = './model_saves/lstm_v2/'\n",
    "model.save(saved_model_path, include_optimizer=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reload and test inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reloaded_model = tf.saved_model.load(saved_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.ops.numpy_ops import np_config\n",
    "np_config.enable_numpy_behavior()\n",
    "\n",
    "def print_my_examples(inputs, results):\n",
    "  for i in range(len(inputs)):\n",
    "    print('input: ', inputs[i], ' : score: ', results.numpy()[i][0], ' : rounded: ', round(results.numpy()[i][0]))\n",
    "  print()\n",
    "\n",
    "\n",
    "examples = [\n",
    "    \"Please, keep talking. I always yawn when I am interested.\", # expect 1\n",
    "    \"Well, what a surprise.\", # expect 1\n",
    "    \"Really, Sherlock? No! You are clever.\", # expect 1\n",
    "    \"The quick brown fox jumps over the lazy dog\", # expect 0\n",
    "    \"Numerous references to the phrase have occurred in movies, television, and books.\" # expect 0\n",
    "]\n",
    "\n",
    "reloaded_results = tf.sigmoid(reloaded_model(tf.constant(examples)))\n",
    "original_results = tf.sigmoid(model(tf.constant(examples)))\n",
    "\n",
    "print('Results from the saved model:')\n",
    "print_my_examples(examples, reloaded_results)\n",
    "print('Results from the model in memory:')\n",
    "print_my_examples(examples, original_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e1779b9e32decb4ffd82f3f39347ae0c85f7e30a4d61c3fccf37932ed55b801a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
