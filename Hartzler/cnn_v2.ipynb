{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, Layer\n",
    "from tensorflow.keras import Model\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>thirtysomething scientists unveil doomsday clo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dem rep. totally nails why congress is falling...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eat your veggies: 9 deliciously different recipes</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>inclement weather prevents liar from getting t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mother comes pretty close to using word 'strea...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  thirtysomething scientists unveil doomsday clo...      1\n",
       "1  dem rep. totally nails why congress is falling...      0\n",
       "2  eat your veggies: 9 deliciously different recipes      0\n",
       "3  inclement weather prevents liar from getting t...      1\n",
       "4  mother comes pretty close to using word 'strea...      1"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_news_headlines = pd.read_json(\"../shared_data/x1.json\")\n",
    "\n",
    "# Adjust news headline data\n",
    "data_news_headlines = data_news_headlines.drop(columns='article_link', axis=1)\n",
    "data_news_headlines = data_news_headlines.rename(columns ={'headline':'text', 'is_sarcastic':'label'})\n",
    "data_news_headlines = data_news_headlines.reindex(columns=['text','label'])\n",
    "data_news_headlines.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I love working midnights tweet</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I hate when I buy a bag of air and there's chi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>my grandad always sounds so ill when i speak t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I realize I'm annoying to everyone, so I won't...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I love when I find these dudes on vine!! #Foll...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0                    I love working midnights tweet       1\n",
       "1  I hate when I buy a bag of air and there's chi...      1\n",
       "2  my grandad always sounds so ill when i speak t...      0\n",
       "3  I realize I'm annoying to everyone, so I won't...      0\n",
       "4  I love when I find these dudes on vine!! #Foll...      1"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tweets = pd.read_csv(\"../shared_data/dataset_csv.csv\")\n",
    "\n",
    "# Adjust tweets data\n",
    "data_tweets = data_tweets.rename(columns={'tweets':'text'})\n",
    "data_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>And of those few months, how long have you bee...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Let the dead man talk. So, why do you think that?</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>What else? Sell it on eBay as \"slightly used.\"</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Good idea, sit with her. Hold her, comfort her...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Well, now that I've given up string theory, I'...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  label\n",
       "5   And of those few months, how long have you bee...    0.0\n",
       "14  Let the dead man talk. So, why do you think that?    0.0\n",
       "18     What else? Sell it on eBay as \"slightly used.\"    0.0\n",
       "24  Good idea, sit with her. Hold her, comfort her...    1.0\n",
       "31  Well, now that I've given up string theory, I'...    0.0"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sitcoms = pd.read_csv(\"../shared_data/mustard++_text.csv\")\n",
    "\n",
    "# Adjust sitcom data\n",
    "data_sitcoms = data_sitcoms.drop(columns=['SCENE','KEY','END_TIME','SPEAKER','SHOW','Sarcasm_Type','Implicit_Emotion','Explicit_Emotion','Valence','Arousal'], axis=1)\n",
    "data_sitcoms = data_sitcoms.rename(columns={'SENTENCE':'text','Sarcasm':'label'})\n",
    "\n",
    "# remove empty label rows\n",
    "for index, row in data_sitcoms.iterrows():\n",
    "    if math.isnan(row['label']):\n",
    "        data_sitcoms = data_sitcoms.drop(index, axis='index')\n",
    "\n",
    "data_sitcoms.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NC and NH.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You do know west teams play against west teams...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>They were underdogs earlier today, but since G...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This meme isn't funny none of the \"new york ni...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I could use one of those tools.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0                                         NC and NH.      0\n",
       "1  You do know west teams play against west teams...      0\n",
       "2  They were underdogs earlier today, but since G...      0\n",
       "3  This meme isn't funny none of the \"new york ni...      0\n",
       "4                    I could use one of those tools.      0"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_reddit = pd.read_csv(\"../shared_data/train-balanced-sarcasm.csv\")\n",
    "\n",
    "# Adjust reddit data\n",
    "data_reddit = data_reddit.drop(columns=['author','subreddit','score','ups','downs','date','created_utc','parent_comment'], axis=1)\n",
    "data_reddit = data_reddit.rename(columns={'comment':'text'})\n",
    "data_reddit = data_reddit.reindex(columns=['text','label'])\n",
    "\n",
    "data_reddit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Install Windows</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Not sure how you guys all feel, but they shoul...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>With a long niqab</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Calm down sjw.</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>xd</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0                                    Install Windows    1.0\n",
       "1  Not sure how you guys all feel, but they shoul...    0.0\n",
       "2                                  With a long niqab    1.0\n",
       "3                                     Calm down sjw.    1.0\n",
       "4                                                 xd    0.0"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine all 4 datasets\n",
    "data = pd.concat([data_news_headlines,data_tweets,data_sitcoms,data_reddit], ignore_index=True)\n",
    "\n",
    "# remove non string (nan) rows\n",
    "for index, row in data.iterrows():\n",
    "    if not type(row['text']) == str:\n",
    "        data = data.drop(index, axis='index')\n",
    "\n",
    "# Shuffle the rows\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1042588 entries, 0 to 1042587\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count    Dtype  \n",
      "---  ------  --------------    -----  \n",
      " 0   text    1042588 non-null  object \n",
      " 1   label   1042588 non-null  float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 15.9+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1042588 entries, 0 to 1042587\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count    Dtype  \n",
      "---  ------  --------------    -----  \n",
      " 0   text    1042588 non-null  object \n",
      " 1   label   1042588 non-null  float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 15.9+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the variables for the model and training/testing processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_size = 400\n",
    "training_size = int(subset_size * 0.2)\n",
    "shuffle_size = subset_size - training_size\n",
    "\n",
    "data_batch_size = 32\n",
    "image_size = (64, 64)\n",
    "word_cloud_font_path = '../shared_data/font/DroidSansMono.ttf'\n",
    "\n",
    "EPOCHS = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomly shuffle the data and select the top subset size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "data = data.head(subset_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create tensorflow dataset tensor objects and split the data between training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (\n",
    "        data['text'][training_size:], \n",
    "        data['label'][training_size:]\n",
    "    )\n",
    ").shuffle(shuffle_size).batch(data_batch_size)\n",
    "\n",
    "test_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (\n",
    "        data['text'][:training_size],\n",
    "        data['label'][:training_size]\n",
    "    )\n",
    ").batch(data_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a custom layer that takes a sentence text tensor and returns a wordcloud of that sentence as an image tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentence2WordCloud(Layer):\n",
    "    def __init__(self) -> None:\n",
    "        super(Sentence2WordCloud, self).__init__()\n",
    "        self.trainable = False\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        output = []\n",
    "        for tensor in inputs:\n",
    "            new_tensor = tf.convert_to_tensor(self.__sentence2wordcloud__(tensor), dtype_hint=tf.float32)\n",
    "            output.append(new_tensor)\n",
    "        return tf.convert_to_tensor(output)\n",
    "    \n",
    "    def __sentence2wordcloud__(self, tensor):\n",
    "        frequencies = self.__freqcount__(tensor)\n",
    "        cloud = WordCloud(width=image_size[0], height=image_size[1], stopwords=[''], min_word_length=1, repeat=True, normalize_plurals=False, include_numbers=True, font_path=word_cloud_font_path, min_font_size=1)\n",
    "        image = cloud.generate_from_frequencies(frequencies)\n",
    "        return image.to_array()\n",
    "    \n",
    "    def __freqcount__(self, tensor):\n",
    "        words = tf.get_static_value(tensor).decode().split()\n",
    "        freq_count = [words.count(k) for k in words]\n",
    "        return dict(zip(words, freq_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"\\ntest model for troubleshooting layer\\n\\nclass MyModel(Model):\\n    def __init__(self) -> None:\\n        super(MyModel, self).__init__()\\n        self.l1 = Sentence2WordCloud()\\n    \\n    def call(self, x):\\n        return self.l1(x)\\n\\nmy_model = MyModel()\\nfor sentences, labels in test_ds:\\n    print(my_model(sentences))\\n'"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\"\n",
    "test model for troubleshooting layer\n",
    "\n",
    "class MyModel(Model):\n",
    "    def __init__(self) -> None:\n",
    "        super(MyModel, self).__init__()\n",
    "        self.l1 = Sentence2WordCloud()\n",
    "    \n",
    "    def call(self, x):\n",
    "        return self.l1(x)\n",
    "\n",
    "my_model = MyModel()\n",
    "for sentences, labels in test_ds:\n",
    "    print(my_model(sentences))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a custom convolution neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CloudyCNN(Model):\n",
    "    def __init__(self) -> None:\n",
    "        super(CloudyCNN, self).__init__()\n",
    "        self.sentence2wordcloud = Sentence2WordCloud()\n",
    "        self.conv1 = Conv2D(164, 3, activation='relu')\n",
    "        self.flatten = Flatten()\n",
    "        self.d1 = Dense(512, activation='relu')\n",
    "        self.d2 = Dense(100, activation='relu')\n",
    "        self.d3 = Dense(10, activation='relu')\n",
    "        self.d4 = Dense(1, activation='sigmoid')\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.sentence2wordcloud(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.d1(x)\n",
    "        x = self.d2(x)\n",
    "        x = self.d3(x)\n",
    "        return self.d4(x)\n",
    "\n",
    "model = CloudyCNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select metrics to measure the loss and the accuracy of the model. These metrics accumulate the values over epochs and then print the overall result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select metrics to measure the loss and the accuracy of the model. These metrics accumulate the values over epochs and then print the overall result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.BinaryCrossentropy(name='train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.BinaryCrossentropy(name='test_accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the training function: using `tf.GradientTape `to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(sentences, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # training=True is only needed if there are layers with different\n",
    "        # behavior during training versus inference (e.g. Dropout).\n",
    "        predictions = model(sentences, training=True)\n",
    "        loss = loss_object(labels, predictions)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "        train_loss(loss)\n",
    "        train_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the testing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(sentences, labels):\n",
    "    # training=False is only needed if there are layers with different\n",
    "    # behavior during training versus inference (e.g. Dropout).\n",
    "    predictions = model(sentences, training=False)\n",
    "    t_loss = loss_object(labels, predictions)\n",
    "\n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and Test the CloudyCNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, "
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    # Reset the metrics at the start of the next epoch\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    test_loss.reset_states()\n",
    "    test_accuracy.reset_states()\n",
    "\n",
    "    print(f'Epoch {epoch + 1}:', end=\" \")\n",
    "    start_time = time.time()\n",
    "\n",
    "    for sentences, labels in train_ds:\n",
    "        train_step(sentences, labels)\n",
    "    \n",
    "    train_time = time.time()\n",
    "\n",
    "    print(\n",
    "        f'Loss: {train_loss.result()}, '\n",
    "        f'Accuracy: {train_accuracy.result() * 0.1}, '\n",
    "        f'Train Time: {round(train_time - start_time)}s',\n",
    "        end=\" \"\n",
    "    )\n",
    "\n",
    "    for test_sentences, test_labels in test_ds:\n",
    "        test_step(test_sentences, test_labels)\n",
    "\n",
    "    test_time = time.time()\n",
    "\n",
    "    print(\n",
    "        f'Test Loss: {test_loss.result()}, '\n",
    "        f'Test Accuracy: {test_accuracy.result() * 0.1}, '\n",
    "        f'Test Time: {round(test_time - train_time)}s, '\n",
    "        f'Epoch Time: {round(test_time - start_time)}s'\n",
    "    )    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\ftn0813\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 2041, in predict_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\ftn0813\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 2027, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\ftn0813\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 2015, in run_step  **\n        outputs = model.predict_step(data)\n    File \"C:\\Users\\ftn0813\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 1983, in predict_step\n        return self(x, training=False)\n    File \"C:\\Users\\ftn0813\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\ftn0813\\AppData\\Local\\Temp\\__autograph_generated_fileeyr78_pg.py\", line 10, in tf__call\n        x = ag__.converted_call(ag__.ld(self).sentence2wordcloud, (ag__.ld(x),), None, fscope)\n    File \"C:\\Users\\ftn0813\\AppData\\Local\\Temp\\__autograph_generated_file6v53uftz.py\", line 24, in tf__call\n        ag__.for_stmt(ag__.ld(inputs), None, loop_body, get_state, set_state, (), {'iterate_names': 'tensor'})\n    File \"C:\\Users\\ftn0813\\AppData\\Local\\Temp\\__autograph_generated_file6v53uftz.py\", line 20, in loop_body\n        new_tensor = ag__.converted_call(ag__.ld(tf).convert_to_tensor, (ag__.converted_call(ag__.ld(self).__sentence2wordcloud__, (ag__.ld(tensor),), None, fscope),), None, fscope)\n    File \"C:\\Users\\ftn0813\\AppData\\Local\\Temp\\__autograph_generated_file3gk6jg83.py\", line 10, in tf____sentence2wordcloud__\n        frequencies = ag__.converted_call(ag__.ld(self).__freqcount__, (ag__.ld(tensor),), None, fscope)\n    File \"C:\\Users\\ftn0813\\AppData\\Local\\Temp\\__autograph_generated_file0l_znj3v.py\", line 10, in tf____freqcount__\n        words = ag__.converted_call(ag__.converted_call(ag__.converted_call(ag__.ld(tensor).numpy, (), None, fscope).decode, (), None, fscope).split, (), None, fscope)\n\n    AttributeError: Exception encountered when calling layer \"cloudy_cnn_5\" \"                 f\"(type CloudyCNN).\n    \n    in user code:\n    \n        File \"C:\\Users\\ftn0813\\AppData\\Local\\Temp\\ipykernel_2844\\442203258.py\", line 13, in call  *\n            x = self.sentence2wordcloud(x)\n        File \"C:\\Users\\ftn0813\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"C:\\Users\\ftn0813\\AppData\\Local\\Temp\\__autograph_generated_file6v53uftz.py\", line 24, in tf__call\n            ag__.for_stmt(ag__.ld(inputs), None, loop_body, get_state, set_state, (), {'iterate_names': 'tensor'})\n        File \"C:\\Users\\ftn0813\\AppData\\Local\\Temp\\__autograph_generated_file6v53uftz.py\", line 20, in loop_body\n            new_tensor = ag__.converted_call(ag__.ld(tf).convert_to_tensor, (ag__.converted_call(ag__.ld(self).__sentence2wordcloud__, (ag__.ld(tensor),), None, fscope),), None, fscope)\n        File \"C:\\Users\\ftn0813\\AppData\\Local\\Temp\\__autograph_generated_file3gk6jg83.py\", line 10, in tf____sentence2wordcloud__\n            frequencies = ag__.converted_call(ag__.ld(self).__freqcount__, (ag__.ld(tensor),), None, fscope)\n        File \"C:\\Users\\ftn0813\\AppData\\Local\\Temp\\__autograph_generated_file0l_znj3v.py\", line 10, in tf____freqcount__\n            words = ag__.converted_call(ag__.converted_call(ag__.converted_call(ag__.ld(tensor).numpy, (), None, fscope).decode, (), None, fscope).split, (), None, fscope)\n    \n        AttributeError: Exception encountered when calling layer \"sentence2_word_cloud_16878\" \"                 f\"(type Sentence2WordCloud).\n        \n        in user code:\n        \n            File \"C:\\Users\\ftn0813\\AppData\\Local\\Temp\\ipykernel_2844\\977086180.py\", line 9, in call  *\n                new_tensor = tf.convert_to_tensor(self.__sentence2wordcloud__(tensor))\n            File \"C:\\Users\\ftn0813\\AppData\\Local\\Temp\\ipykernel_2844\\2642161880.py\", line 14, in __sentence2wordcloud__  *\n                frequencies = self.__freqcount__(tensor)\n            File \"C:\\Users\\ftn0813\\AppData\\Local\\Temp\\ipykernel_2844\\977086180.py\", line 20, in __freqcount__  *\n                words = tensor.numpy().decode().split()\n        \n            AttributeError: 'Tensor' object has no attribute 'numpy'\n        \n        \n        Call arguments received by layer \"sentence2_word_cloud_16878\" \"                 f\"(type Sentence2WordCloud):\n          • inputs=tf.Tensor(shape=(None,), dtype=string)\n    \n    \n    Call arguments received by layer \"cloudy_cnn_5\" \"                 f\"(type CloudyCNN):\n      • x=tf.Tensor(shape=(None,), dtype=string)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ftn0813\\Documents\\GitHub\\ScannerTraining\\Hartzler\\cnn_v2.ipynb Cell 29\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ftn0813/Documents/GitHub/ScannerTraining/Hartzler/cnn_v2.ipynb#X25sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m new_sentences \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mNow we know why some animals eat their own children.\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mgame of thrones season finale showing this sunday night\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mPlease, keep talking. I always yawn when I am interested.\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ftn0813/Documents/GitHub/ScannerTraining/Hartzler/cnn_v2.ipynb#X25sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mround\u001b[39m(model\u001b[39m.\u001b[39;49mpredict(new_sentences)))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filezvwe8e9b.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__predict_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_fileeyr78_pg.py:10\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m      8\u001b[0m do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m      9\u001b[0m retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefinedReturnValue()\n\u001b[1;32m---> 10\u001b[0m x \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39msentence2wordcloud, (ag__\u001b[39m.\u001b[39mld(x),), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     11\u001b[0m x \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mconv1, (ag__\u001b[39m.\u001b[39mld(x),), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     12\u001b[0m x \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mflatten, (ag__\u001b[39m.\u001b[39mld(x),), \u001b[39mNone\u001b[39;00m, fscope)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file6v53uftz.py:24\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     22\u001b[0m new_tensor \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefined(\u001b[39m'\u001b[39m\u001b[39mnew_tensor\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     23\u001b[0m tensor \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefined(\u001b[39m'\u001b[39m\u001b[39mtensor\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 24\u001b[0m ag__\u001b[39m.\u001b[39mfor_stmt(ag__\u001b[39m.\u001b[39mld(inputs), \u001b[39mNone\u001b[39;00m, loop_body, get_state, set_state, (), {\u001b[39m'\u001b[39m\u001b[39miterate_names\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mtensor\u001b[39m\u001b[39m'\u001b[39m})\n\u001b[0;32m     25\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     26\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file6v53uftz.py:20\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call.<locals>.loop_body\u001b[1;34m(itr)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mloop_body\u001b[39m(itr):\n\u001b[0;32m     19\u001b[0m     tensor \u001b[39m=\u001b[39m itr\n\u001b[1;32m---> 20\u001b[0m     new_tensor \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mconvert_to_tensor, (ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(\u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m__sentence2wordcloud__, (ag__\u001b[39m.\u001b[39;49mld(tensor),), \u001b[39mNone\u001b[39;49;00m, fscope),), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     21\u001b[0m     ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(output)\u001b[39m.\u001b[39mappend, (ag__\u001b[39m.\u001b[39mld(new_tensor),), \u001b[39mNone\u001b[39;00m, fscope)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file3gk6jg83.py:10\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf____sentence2wordcloud__\u001b[1;34m(self, tensor)\u001b[0m\n\u001b[0;32m      8\u001b[0m do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m      9\u001b[0m retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefinedReturnValue()\n\u001b[1;32m---> 10\u001b[0m frequencies \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m__freqcount__, (ag__\u001b[39m.\u001b[39mld(tensor),), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     11\u001b[0m cloud \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(WordCloud), (), \u001b[39mdict\u001b[39m(width\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(image_size)[\u001b[39m0\u001b[39m], height\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(image_size)[\u001b[39m1\u001b[39m], stopwords\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m], min_word_length\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, repeat\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, normalize_plurals\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, include_numbers\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, font_path\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(word_cloud_font_path)), fscope)\n\u001b[0;32m     12\u001b[0m image \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(cloud)\u001b[39m.\u001b[39mgenerate_from_frequencies, (ag__\u001b[39m.\u001b[39mld(frequencies),), \u001b[39mNone\u001b[39;00m, fscope)\u001b[39m.\u001b[39mto_array, (), \u001b[39mNone\u001b[39;00m, fscope)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file0l_znj3v.py:10\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf____freqcount__\u001b[1;34m(self, tensor)\u001b[0m\n\u001b[0;32m      8\u001b[0m do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m      9\u001b[0m retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefinedReturnValue()\n\u001b[1;32m---> 10\u001b[0m words \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tensor)\u001b[39m.\u001b[39mnumpy, (), \u001b[39mNone\u001b[39;00m, fscope)\u001b[39m.\u001b[39mdecode, (), \u001b[39mNone\u001b[39;00m, fscope)\u001b[39m.\u001b[39msplit, (), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     11\u001b[0m freq_count \u001b[39m=\u001b[39m [ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(words)\u001b[39m.\u001b[39mcount, (ag__\u001b[39m.\u001b[39mld(k),), \u001b[39mNone\u001b[39;00m, fscope) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m ag__\u001b[39m.\u001b[39mld(words)]\n\u001b[0;32m     12\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[1;31mAttributeError\u001b[0m: in user code:\n\n    File \"C:\\Users\\ftn0813\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 2041, in predict_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\ftn0813\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 2027, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\ftn0813\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 2015, in run_step  **\n        outputs = model.predict_step(data)\n    File \"C:\\Users\\ftn0813\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 1983, in predict_step\n        return self(x, training=False)\n    File \"C:\\Users\\ftn0813\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\ftn0813\\AppData\\Local\\Temp\\__autograph_generated_fileeyr78_pg.py\", line 10, in tf__call\n        x = ag__.converted_call(ag__.ld(self).sentence2wordcloud, (ag__.ld(x),), None, fscope)\n    File \"C:\\Users\\ftn0813\\AppData\\Local\\Temp\\__autograph_generated_file6v53uftz.py\", line 24, in tf__call\n        ag__.for_stmt(ag__.ld(inputs), None, loop_body, get_state, set_state, (), {'iterate_names': 'tensor'})\n    File \"C:\\Users\\ftn0813\\AppData\\Local\\Temp\\__autograph_generated_file6v53uftz.py\", line 20, in loop_body\n        new_tensor = ag__.converted_call(ag__.ld(tf).convert_to_tensor, (ag__.converted_call(ag__.ld(self).__sentence2wordcloud__, (ag__.ld(tensor),), None, fscope),), None, fscope)\n    File \"C:\\Users\\ftn0813\\AppData\\Local\\Temp\\__autograph_generated_file3gk6jg83.py\", line 10, in tf____sentence2wordcloud__\n        frequencies = ag__.converted_call(ag__.ld(self).__freqcount__, (ag__.ld(tensor),), None, fscope)\n    File \"C:\\Users\\ftn0813\\AppData\\Local\\Temp\\__autograph_generated_file0l_znj3v.py\", line 10, in tf____freqcount__\n        words = ag__.converted_call(ag__.converted_call(ag__.converted_call(ag__.ld(tensor).numpy, (), None, fscope).decode, (), None, fscope).split, (), None, fscope)\n\n    AttributeError: Exception encountered when calling layer \"cloudy_cnn_5\" \"                 f\"(type CloudyCNN).\n    \n    in user code:\n    \n        File \"C:\\Users\\ftn0813\\AppData\\Local\\Temp\\ipykernel_2844\\442203258.py\", line 13, in call  *\n            x = self.sentence2wordcloud(x)\n        File \"C:\\Users\\ftn0813\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"C:\\Users\\ftn0813\\AppData\\Local\\Temp\\__autograph_generated_file6v53uftz.py\", line 24, in tf__call\n            ag__.for_stmt(ag__.ld(inputs), None, loop_body, get_state, set_state, (), {'iterate_names': 'tensor'})\n        File \"C:\\Users\\ftn0813\\AppData\\Local\\Temp\\__autograph_generated_file6v53uftz.py\", line 20, in loop_body\n            new_tensor = ag__.converted_call(ag__.ld(tf).convert_to_tensor, (ag__.converted_call(ag__.ld(self).__sentence2wordcloud__, (ag__.ld(tensor),), None, fscope),), None, fscope)\n        File \"C:\\Users\\ftn0813\\AppData\\Local\\Temp\\__autograph_generated_file3gk6jg83.py\", line 10, in tf____sentence2wordcloud__\n            frequencies = ag__.converted_call(ag__.ld(self).__freqcount__, (ag__.ld(tensor),), None, fscope)\n        File \"C:\\Users\\ftn0813\\AppData\\Local\\Temp\\__autograph_generated_file0l_znj3v.py\", line 10, in tf____freqcount__\n            words = ag__.converted_call(ag__.converted_call(ag__.converted_call(ag__.ld(tensor).numpy, (), None, fscope).decode, (), None, fscope).split, (), None, fscope)\n    \n        AttributeError: Exception encountered when calling layer \"sentence2_word_cloud_16878\" \"                 f\"(type Sentence2WordCloud).\n        \n        in user code:\n        \n            File \"C:\\Users\\ftn0813\\AppData\\Local\\Temp\\ipykernel_2844\\977086180.py\", line 9, in call  *\n                new_tensor = tf.convert_to_tensor(self.__sentence2wordcloud__(tensor))\n            File \"C:\\Users\\ftn0813\\AppData\\Local\\Temp\\ipykernel_2844\\2642161880.py\", line 14, in __sentence2wordcloud__  *\n                frequencies = self.__freqcount__(tensor)\n            File \"C:\\Users\\ftn0813\\AppData\\Local\\Temp\\ipykernel_2844\\977086180.py\", line 20, in __freqcount__  *\n                words = tensor.numpy().decode().split()\n        \n            AttributeError: 'Tensor' object has no attribute 'numpy'\n        \n        \n        Call arguments received by layer \"sentence2_word_cloud_16878\" \"                 f\"(type Sentence2WordCloud):\n          • inputs=tf.Tensor(shape=(None,), dtype=string)\n    \n    \n    Call arguments received by layer \"cloudy_cnn_5\" \"                 f\"(type CloudyCNN):\n      • x=tf.Tensor(shape=(None,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "new_sentences = [\"Now we know why some animals eat their own children.\", \"game of thrones season finale showing this sunday night\",\"Please, keep talking. I always yawn when I am interested.\"]\n",
    "new_dataset = tf.data.Dataset.from_tensor_slices(new_sentences)\n",
    "for sentence in new_dataset:\n",
    "    print(round(model.predict(sentence)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./model_saves/cnn/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "deb4792152b8b9767403eeef0a1b0f34b83d442136ccee9184cd7d1131f09aa0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
