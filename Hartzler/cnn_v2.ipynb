{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, Layer\n",
    "from tensorflow.keras import Model\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>thirtysomething scientists unveil doomsday clo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dem rep. totally nails why congress is falling...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eat your veggies: 9 deliciously different recipes</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>inclement weather prevents liar from getting t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mother comes pretty close to using word 'strea...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  thirtysomething scientists unveil doomsday clo...      1\n",
       "1  dem rep. totally nails why congress is falling...      0\n",
       "2  eat your veggies: 9 deliciously different recipes      0\n",
       "3  inclement weather prevents liar from getting t...      1\n",
       "4  mother comes pretty close to using word 'strea...      1"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_news_headlines = pd.read_json(\"../shared_data/x1.json\")\n",
    "\n",
    "# Adjust news headline data\n",
    "data_news_headlines = data_news_headlines.drop(columns='article_link', axis=1)\n",
    "data_news_headlines = data_news_headlines.rename(columns ={'headline':'text', 'is_sarcastic':'label'})\n",
    "data_news_headlines = data_news_headlines.reindex(columns=['text','label'])\n",
    "data_news_headlines.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I love working midnights tweet</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I hate when I buy a bag of air and there's chi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>my grandad always sounds so ill when i speak t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I realize I'm annoying to everyone, so I won't...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I love when I find these dudes on vine!! #Foll...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0                    I love working midnights tweet       1\n",
       "1  I hate when I buy a bag of air and there's chi...      1\n",
       "2  my grandad always sounds so ill when i speak t...      0\n",
       "3  I realize I'm annoying to everyone, so I won't...      0\n",
       "4  I love when I find these dudes on vine!! #Foll...      1"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tweets = pd.read_csv(\"../shared_data/dataset_csv.csv\")\n",
    "\n",
    "# Adjust tweets data\n",
    "data_tweets = data_tweets.rename(columns={'tweets':'text'})\n",
    "data_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>And of those few months, how long have you bee...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Let the dead man talk. So, why do you think that?</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>What else? Sell it on eBay as \"slightly used.\"</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Good idea, sit with her. Hold her, comfort her...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Well, now that I've given up string theory, I'...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  label\n",
       "5   And of those few months, how long have you bee...    0.0\n",
       "14  Let the dead man talk. So, why do you think that?    0.0\n",
       "18     What else? Sell it on eBay as \"slightly used.\"    0.0\n",
       "24  Good idea, sit with her. Hold her, comfort her...    1.0\n",
       "31  Well, now that I've given up string theory, I'...    0.0"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sitcoms = pd.read_csv(\"../shared_data/mustard++_text.csv\")\n",
    "\n",
    "# Adjust sitcom data\n",
    "data_sitcoms = data_sitcoms.drop(columns=['SCENE','KEY','END_TIME','SPEAKER','SHOW','Sarcasm_Type','Implicit_Emotion','Explicit_Emotion','Valence','Arousal'], axis=1)\n",
    "data_sitcoms = data_sitcoms.rename(columns={'SENTENCE':'text','Sarcasm':'label'})\n",
    "\n",
    "# remove empty label rows\n",
    "for index, row in data_sitcoms.iterrows():\n",
    "    if math.isnan(row['label']):\n",
    "        data_sitcoms = data_sitcoms.drop(index, axis='index')\n",
    "\n",
    "data_sitcoms.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NC and NH.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You do know west teams play against west teams...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>They were underdogs earlier today, but since G...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This meme isn't funny none of the \"new york ni...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I could use one of those tools.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0                                         NC and NH.      0\n",
       "1  You do know west teams play against west teams...      0\n",
       "2  They were underdogs earlier today, but since G...      0\n",
       "3  This meme isn't funny none of the \"new york ni...      0\n",
       "4                    I could use one of those tools.      0"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_reddit = pd.read_csv(\"../shared_data/train-balanced-sarcasm.csv\")\n",
    "\n",
    "# Adjust reddit data\n",
    "data_reddit = data_reddit.drop(columns=['author','subreddit','score','ups','downs','date','created_utc','parent_comment'], axis=1)\n",
    "data_reddit = data_reddit.rename(columns={'comment':'text'})\n",
    "data_reddit = data_reddit.reindex(columns=['text','label'])\n",
    "\n",
    "data_reddit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Install Windows</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Not sure how you guys all feel, but they shoul...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>With a long niqab</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Calm down sjw.</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>xd</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0                                    Install Windows    1.0\n",
       "1  Not sure how you guys all feel, but they shoul...    0.0\n",
       "2                                  With a long niqab    1.0\n",
       "3                                     Calm down sjw.    1.0\n",
       "4                                                 xd    0.0"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine all 4 datasets\n",
    "data = pd.concat([data_news_headlines,data_tweets,data_sitcoms,data_reddit], ignore_index=True)\n",
    "\n",
    "# remove non string (nan) rows\n",
    "for index, row in data.iterrows():\n",
    "    if not type(row['text']) == str:\n",
    "        data = data.drop(index, axis='index')\n",
    "\n",
    "# Shuffle the rows\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1042588 entries, 0 to 1042587\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count    Dtype  \n",
      "---  ------  --------------    -----  \n",
      " 0   text    1042588 non-null  object \n",
      " 1   label   1042588 non-null  float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 15.9+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1042588 entries, 0 to 1042587\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count    Dtype  \n",
      "---  ------  --------------    -----  \n",
      " 0   text    1042588 non-null  object \n",
      " 1   label   1042588 non-null  float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 15.9+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the variables for the model and training/testing processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_size = 400\n",
    "training_size = int(subset_size * 0.2)\n",
    "shuffle_size = subset_size - training_size\n",
    "\n",
    "data_batch_size = 32\n",
    "image_size = (64, 64)\n",
    "word_cloud_font_path = '../shared_data/font/DroidSansMono.ttf'\n",
    "\n",
    "EPOCHS = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomly shuffle the data and select the top subset size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "data = data.head(subset_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create tensorflow dataset tensor objects and split the data between training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (\n",
    "        data['text'][training_size:], \n",
    "        data['label'][training_size:]\n",
    "    )\n",
    ").shuffle(shuffle_size).batch(data_batch_size)\n",
    "\n",
    "test_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (\n",
    "        data['text'][:training_size],\n",
    "        data['label'][:training_size]\n",
    "    )\n",
    ").batch(data_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a custom layer that takes a sentence text tensor and returns a wordcloud of that sentence as an image tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentence2WordCloud(Layer):\n",
    "    def __init__(self) -> None:\n",
    "        super(Sentence2WordCloud, self).__init__()\n",
    "        self.trainable = False\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        output = []\n",
    "        for tensor in inputs:\n",
    "            new_tensor = tf.convert_to_tensor(self.__sentence2wordcloud__(tensor), dtype_hint=tf.float32)\n",
    "            output.append(new_tensor)\n",
    "        return tf.convert_to_tensor(output)\n",
    "    \n",
    "    def __sentence2wordcloud__(self, tensor):\n",
    "        frequencies = self.__freqcount__(tensor)\n",
    "        cloud = WordCloud(width=image_size[0], height=image_size[1], stopwords=[''], min_word_length=1, repeat=True, normalize_plurals=False, include_numbers=True, font_path=word_cloud_font_path, min_font_size=1)\n",
    "        image = cloud.generate_from_frequencies(frequencies)\n",
    "        return image.to_array()\n",
    "    \n",
    "    def __freqcount__(self, tensor):\n",
    "        words = tf.get_static_value(tensor).decode().split()\n",
    "        freq_count = [words.count(k) for k in words]\n",
    "        return dict(zip(words, freq_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"\\ntest model for troubleshooting layer\\n\\nclass MyModel(Model):\\n    def __init__(self) -> None:\\n        super(MyModel, self).__init__()\\n        self.l1 = Sentence2WordCloud()\\n    \\n    def call(self, x):\\n        return self.l1(x)\\n\\nmy_model = MyModel()\\nfor sentences, labels in test_ds:\\n    print(my_model(sentences))\\n'"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\"\n",
    "test model for troubleshooting layer\n",
    "\n",
    "class MyModel(Model):\n",
    "    def __init__(self) -> None:\n",
    "        super(MyModel, self).__init__()\n",
    "        self.l1 = Sentence2WordCloud()\n",
    "    \n",
    "    def call(self, x):\n",
    "        return self.l1(x)\n",
    "\n",
    "my_model = MyModel()\n",
    "for sentences, labels in test_ds:\n",
    "    print(my_model(sentences))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a custom convolution neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CloudyCNN(Model):\n",
    "    def __init__(self) -> None:\n",
    "        super(CloudyCNN, self).__init__()\n",
    "        self.sentence2wordcloud = Sentence2WordCloud()\n",
    "        self.conv1 = Conv2D(164, 3, activation='relu')\n",
    "        self.flatten = Flatten()\n",
    "        self.d1 = Dense(512, activation='relu')\n",
    "        self.d2 = Dense(100, activation='relu')\n",
    "        self.d3 = Dense(10, activation='relu')\n",
    "        self.d4 = Dense(1, activation='sigmoid')\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.sentence2wordcloud(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.d1(x)\n",
    "        x = self.d2(x)\n",
    "        x = self.d3(x)\n",
    "        return self.d4(x)\n",
    "\n",
    "model = CloudyCNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select metrics to measure the loss and the accuracy of the model. These metrics accumulate the values over epochs and then print the overall result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select metrics to measure the loss and the accuracy of the model. These metrics accumulate the values over epochs and then print the overall result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.BinaryAccuracy(name='train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.BinaryAccuracy(name='test_accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the training function: using `tf.GradientTape `to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(sentences, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # training=True is only needed if there are layers with different\n",
    "        # behavior during training versus inference (e.g. Dropout).\n",
    "        predictions = model(sentences, training=True)\n",
    "        loss = loss_object(labels, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the testing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(sentences, labels):\n",
    "    # training=False is only needed if there are layers with different\n",
    "    # behavior during training versus inference (e.g. Dropout).\n",
    "    predictions = model(sentences, training=False)\n",
    "    t_loss = loss_object(labels, predictions)\n",
    "\n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and Test the CloudyCNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss: 0.6931471824645996, Accuracy: 0.4781250059604645, Train Time: 54s Test Loss: 0.6931471824645996, Test Accuracy: 0.512499988079071, Test Time: 11s, Epoch Time: 64s\n",
      "Epoch 2: Loss: 0.6931471824645996, Accuracy: 0.4781250059604645, Train Time: 60s Test Loss: 0.6931471824645996, Test Accuracy: 0.512499988079071, Test Time: 11s, Epoch Time: 71s\n",
      "Epoch 3: "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ftn0813\\Documents\\GitHub\\ScannerTraining\\Hartzler\\cnn_v2.ipynb Cell 31\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ftn0813/Documents/GitHub/ScannerTraining/Hartzler/cnn_v2.ipynb#X51sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ftn0813/Documents/GitHub/ScannerTraining/Hartzler/cnn_v2.ipynb#X51sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mfor\u001b[39;00m sentences, labels \u001b[39min\u001b[39;00m train_ds:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ftn0813/Documents/GitHub/ScannerTraining/Hartzler/cnn_v2.ipynb#X51sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     train_step(sentences, labels)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ftn0813/Documents/GitHub/ScannerTraining/Hartzler/cnn_v2.ipynb#X51sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m train_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ftn0813/Documents/GitHub/ScannerTraining/Hartzler/cnn_v2.ipynb#X51sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mprint\u001b[39m(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ftn0813/Documents/GitHub/ScannerTraining/Hartzler/cnn_v2.ipynb#X51sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mLoss: \u001b[39m\u001b[39m{\u001b[39;00mtrain_loss\u001b[39m.\u001b[39mresult()\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ftn0813/Documents/GitHub/ScannerTraining/Hartzler/cnn_v2.ipynb#X51sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mAccuracy: \u001b[39m\u001b[39m{\u001b[39;00mtrain_accuracy\u001b[39m.\u001b[39mresult()\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ftn0813/Documents/GitHub/ScannerTraining/Hartzler/cnn_v2.ipynb#X51sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTrain Time: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mround\u001b[39m(train_time \u001b[39m-\u001b[39m start_time)\u001b[39m}\u001b[39;00m\u001b[39ms\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ftn0813/Documents/GitHub/ScannerTraining/Hartzler/cnn_v2.ipynb#X51sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     end\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ftn0813/Documents/GitHub/ScannerTraining/Hartzler/cnn_v2.ipynb#X51sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\def_function.py:892\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    890\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_functions_eagerly:\n\u001b[0;32m    891\u001b[0m   \u001b[39mwith\u001b[39;00m trace\u001b[39m.\u001b[39mTrace(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_name, tf_function_call\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39meager\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 892\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_python_function(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    894\u001b[0m \u001b[39m# Only count the statistics the first time, before initialization took\u001b[39;00m\n\u001b[0;32m    895\u001b[0m \u001b[39m# place.\u001b[39;00m\n\u001b[0;32m    896\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_created_variables \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;32mc:\\Users\\ftn0813\\Documents\\GitHub\\ScannerTraining\\Hartzler\\cnn_v2.ipynb Cell 31\u001b[0m in \u001b[0;36mtrain_step\u001b[1;34m(sentences, labels)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ftn0813/Documents/GitHub/ScannerTraining/Hartzler/cnn_v2.ipynb#X51sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m@tf\u001b[39m\u001b[39m.\u001b[39mfunction\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ftn0813/Documents/GitHub/ScannerTraining/Hartzler/cnn_v2.ipynb#X51sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain_step\u001b[39m(sentences, labels):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ftn0813/Documents/GitHub/ScannerTraining/Hartzler/cnn_v2.ipynb#X51sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mGradientTape() \u001b[39mas\u001b[39;00m tape:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ftn0813/Documents/GitHub/ScannerTraining/Hartzler/cnn_v2.ipynb#X51sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m         \u001b[39m# training=True is only needed if there are layers with different\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ftn0813/Documents/GitHub/ScannerTraining/Hartzler/cnn_v2.ipynb#X51sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m         \u001b[39m# behavior during training versus inference (e.g. Dropout).\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ftn0813/Documents/GitHub/ScannerTraining/Hartzler/cnn_v2.ipynb#X51sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m         predictions \u001b[39m=\u001b[39m model(sentences, training\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ftn0813/Documents/GitHub/ScannerTraining/Hartzler/cnn_v2.ipynb#X51sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m         loss \u001b[39m=\u001b[39m loss_object(labels, predictions)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ftn0813/Documents/GitHub/ScannerTraining/Hartzler/cnn_v2.ipynb#X51sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     gradients \u001b[39m=\u001b[39m tape\u001b[39m.\u001b[39mgradient(loss, model\u001b[39m.\u001b[39mtrainable_variables)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py:557\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    553\u001b[0m         \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(inputs, \u001b[39m*\u001b[39mcopied_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcopied_kwargs)\n\u001b[0;32m    555\u001b[0m     layout_map_lib\u001b[39m.\u001b[39m_map_subclass_model_variable(\u001b[39mself\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_layout_map)\n\u001b[1;32m--> 557\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\base_layer.py:1097\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1092\u001b[0m     inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[0;32m   1094\u001b[0m \u001b[39mwith\u001b[39;00m autocast_variable\u001b[39m.\u001b[39menable_auto_cast_variables(\n\u001b[0;32m   1095\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_dtype_object\n\u001b[0;32m   1096\u001b[0m ):\n\u001b[1;32m-> 1097\u001b[0m     outputs \u001b[39m=\u001b[39m call_fn(inputs, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1099\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_activity_regularizer:\n\u001b[0;32m   1100\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py:96\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     94\u001b[0m bound_signature \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 96\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     97\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     98\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39m_keras_call_info_injected\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m     99\u001b[0m         \u001b[39m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\ftn0813\\Documents\\GitHub\\ScannerTraining\\Hartzler\\cnn_v2.ipynb Cell 31\u001b[0m in \u001b[0;36mCloudyCNN.call\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ftn0813/Documents/GitHub/ScannerTraining/Hartzler/cnn_v2.ipynb#X51sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ftn0813/Documents/GitHub/ScannerTraining/Hartzler/cnn_v2.ipynb#X51sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msentence2wordcloud(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ftn0813/Documents/GitHub/ScannerTraining/Hartzler/cnn_v2.ipynb#X51sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ftn0813/Documents/GitHub/ScannerTraining/Hartzler/cnn_v2.ipynb#X51sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mflatten(x)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\base_layer.py:1097\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1092\u001b[0m     inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[0;32m   1094\u001b[0m \u001b[39mwith\u001b[39;00m autocast_variable\u001b[39m.\u001b[39menable_auto_cast_variables(\n\u001b[0;32m   1095\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_dtype_object\n\u001b[0;32m   1096\u001b[0m ):\n\u001b[1;32m-> 1097\u001b[0m     outputs \u001b[39m=\u001b[39m call_fn(inputs, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1099\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_activity_regularizer:\n\u001b[0;32m   1100\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py:96\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     94\u001b[0m bound_signature \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 96\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     97\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     98\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39m_keras_call_info_injected\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m     99\u001b[0m         \u001b[39m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\ftn0813\\Documents\\GitHub\\ScannerTraining\\Hartzler\\cnn_v2.ipynb Cell 31\u001b[0m in \u001b[0;36mSentence2WordCloud.call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ftn0813/Documents/GitHub/ScannerTraining/Hartzler/cnn_v2.ipynb#X51sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m output \u001b[39m=\u001b[39m []\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ftn0813/Documents/GitHub/ScannerTraining/Hartzler/cnn_v2.ipynb#X51sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfor\u001b[39;00m tensor \u001b[39min\u001b[39;00m inputs:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ftn0813/Documents/GitHub/ScannerTraining/Hartzler/cnn_v2.ipynb#X51sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     new_tensor \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mconvert_to_tensor(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__sentence2wordcloud__(tensor), dtype_hint\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mfloat32)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ftn0813/Documents/GitHub/ScannerTraining/Hartzler/cnn_v2.ipynb#X51sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     output\u001b[39m.\u001b[39mappend(new_tensor)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ftn0813/Documents/GitHub/ScannerTraining/Hartzler/cnn_v2.ipynb#X51sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39mconvert_to_tensor(output)\n",
      "\u001b[1;32mc:\\Users\\ftn0813\\Documents\\GitHub\\ScannerTraining\\Hartzler\\cnn_v2.ipynb Cell 31\u001b[0m in \u001b[0;36mSentence2WordCloud.__sentence2wordcloud__\u001b[1;34m(self, tensor)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ftn0813/Documents/GitHub/ScannerTraining/Hartzler/cnn_v2.ipynb#X51sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m frequencies \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__freqcount__(tensor)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ftn0813/Documents/GitHub/ScannerTraining/Hartzler/cnn_v2.ipynb#X51sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m cloud \u001b[39m=\u001b[39m WordCloud(width\u001b[39m=\u001b[39mimage_size[\u001b[39m0\u001b[39m], height\u001b[39m=\u001b[39mimage_size[\u001b[39m1\u001b[39m], stopwords\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m], min_word_length\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, repeat\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, normalize_plurals\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, include_numbers\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, font_path\u001b[39m=\u001b[39mword_cloud_font_path, min_font_size\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ftn0813/Documents/GitHub/ScannerTraining/Hartzler/cnn_v2.ipynb#X51sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m image \u001b[39m=\u001b[39m cloud\u001b[39m.\u001b[39;49mgenerate_from_frequencies(frequencies)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ftn0813/Documents/GitHub/ScannerTraining/Hartzler/cnn_v2.ipynb#X51sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mreturn\u001b[39;00m image\u001b[39m.\u001b[39mto_array()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\wordcloud\\wordcloud.py:548\u001b[0m, in \u001b[0;36mWordCloud.generate_from_frequencies\u001b[1;34m(self, frequencies, max_font_size)\u001b[0m\n\u001b[0;32m    545\u001b[0m         img_array \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(img_grey) \u001b[39m+\u001b[39m boolean_mask\n\u001b[0;32m    546\u001b[0m     \u001b[39m# recompute bottom right\u001b[39;00m\n\u001b[0;32m    547\u001b[0m     \u001b[39m# the order of the cumsum's is important for speed ?!\u001b[39;00m\n\u001b[1;32m--> 548\u001b[0m     occupancy\u001b[39m.\u001b[39;49mupdate(img_array, x, y)\n\u001b[0;32m    549\u001b[0m     last_freq \u001b[39m=\u001b[39m freq\n\u001b[0;32m    551\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayout_ \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(frequencies, font_sizes, positions,\n\u001b[0;32m    552\u001b[0m                         orientations, colors))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\wordcloud\\wordcloud.py:67\u001b[0m, in \u001b[0;36mIntegralOccupancyMap.update\u001b[1;34m(self, img_array, pos_x, pos_y)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[39mif\u001b[39;00m pos_y \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     65\u001b[0m     partial_integral \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintegral[pos_x:, pos_y \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m][:, np\u001b[39m.\u001b[39mnewaxis]\n\u001b[1;32m---> 67\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintegral[pos_x:, pos_y:] \u001b[39m=\u001b[39m partial_integral\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    # Reset the metrics at the start of the next epoch\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    test_loss.reset_states()\n",
    "    test_accuracy.reset_states()\n",
    "\n",
    "    print(f'Epoch {epoch + 1}:', end=\" \")\n",
    "    start_time = time.time()\n",
    "\n",
    "    for sentences, labels in train_ds:\n",
    "        train_step(sentences, labels)\n",
    "    \n",
    "    train_time = time.time()\n",
    "\n",
    "    print(\n",
    "        f'Loss: {train_loss.result()}, '\n",
    "        f'Accuracy: {train_accuracy.result()}, '\n",
    "        f'Train Time: {round(train_time - start_time)}s',\n",
    "        end=\" \"\n",
    "    )\n",
    "\n",
    "    for test_sentences, test_labels in test_ds:\n",
    "        test_step(test_sentences, test_labels)\n",
    "\n",
    "    test_time = time.time()\n",
    "\n",
    "    print(\n",
    "        f'Test Loss: {test_loss.result()}, '\n",
    "        f'Test Accuracy: {test_accuracy.result()}, '\n",
    "        f'Test Time: {round(test_time - train_time)}s, '\n",
    "        f'Epoch Time: {round(test_time - start_time)}s'\n",
    "    )    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\ftn0813\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 2041, in predict_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\ftn0813\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 2027, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\ftn0813\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 2015, in run_step  **\n        outputs = model.predict_step(data)\n    File \"C:\\Users\\ftn0813\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 1983, in predict_step\n        return self(x, training=False)\n    File \"C:\\Users\\ftn0813\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\ftn0813\\AppData\\Local\\Temp\\__autograph_generated_fileeyr78_pg.py\", line 10, in tf__call\n        x = ag__.converted_call(ag__.ld(self).sentence2wordcloud, (ag__.ld(x),), None, fscope)\n    File \"C:\\Users\\ftn0813\\AppData\\Local\\Temp\\__autograph_generated_file6v53uftz.py\", line 24, in tf__call\n        ag__.for_stmt(ag__.ld(inputs), None, loop_body, get_state, set_state, (), {'iterate_names': 'tensor'})\n    File \"C:\\Users\\ftn0813\\AppData\\Local\\Temp\\__autograph_generated_file6v53uftz.py\", line 20, in loop_body\n        new_tensor = ag__.converted_call(ag__.ld(tf).convert_to_tensor, (ag__.converted_call(ag__.ld(self).__sentence2wordcloud__, (ag__.ld(tensor),), None, fscope),), None, fscope)\n    File \"C:\\Users\\ftn0813\\AppData\\Local\\Temp\\__autograph_generated_file3gk6jg83.py\", line 10, in tf____sentence2wordcloud__\n        frequencies = ag__.converted_call(ag__.ld(self).__freqcount__, (ag__.ld(tensor),), None, fscope)\n    File \"C:\\Users\\ftn0813\\AppData\\Local\\Temp\\__autograph_generated_file0l_znj3v.py\", line 10, in tf____freqcount__\n        words = ag__.converted_call(ag__.converted_call(ag__.converted_call(ag__.ld(tensor).numpy, (), None, fscope).decode, (), None, fscope).split, (), None, fscope)\n\n    AttributeError: Exception encountered when calling layer \"cloudy_cnn_5\" \"                 f\"(type CloudyCNN).\n    \n    in user code:\n    \n        File \"C:\\Users\\ftn0813\\AppData\\Local\\Temp\\ipykernel_2844\\442203258.py\", line 13, in call  *\n            x = self.sentence2wordcloud(x)\n        File \"C:\\Users\\ftn0813\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"C:\\Users\\ftn0813\\AppData\\Local\\Temp\\__autograph_generated_file6v53uftz.py\", line 24, in tf__call\n            ag__.for_stmt(ag__.ld(inputs), None, loop_body, get_state, set_state, (), {'iterate_names': 'tensor'})\n        File \"C:\\Users\\ftn0813\\AppData\\Local\\Temp\\__autograph_generated_file6v53uftz.py\", line 20, in loop_body\n            new_tensor = ag__.converted_call(ag__.ld(tf).convert_to_tensor, (ag__.converted_call(ag__.ld(self).__sentence2wordcloud__, (ag__.ld(tensor),), None, fscope),), None, fscope)\n        File \"C:\\Users\\ftn0813\\AppData\\Local\\Temp\\__autograph_generated_file3gk6jg83.py\", line 10, in tf____sentence2wordcloud__\n            frequencies = ag__.converted_call(ag__.ld(self).__freqcount__, (ag__.ld(tensor),), None, fscope)\n        File \"C:\\Users\\ftn0813\\AppData\\Local\\Temp\\__autograph_generated_file0l_znj3v.py\", line 10, in tf____freqcount__\n            words = ag__.converted_call(ag__.converted_call(ag__.converted_call(ag__.ld(tensor).numpy, (), None, fscope).decode, (), None, fscope).split, (), None, fscope)\n    \n        AttributeError: Exception encountered when calling layer \"sentence2_word_cloud_16878\" \"                 f\"(type Sentence2WordCloud).\n        \n        in user code:\n        \n            File \"C:\\Users\\ftn0813\\AppData\\Local\\Temp\\ipykernel_2844\\977086180.py\", line 9, in call  *\n                new_tensor = tf.convert_to_tensor(self.__sentence2wordcloud__(tensor))\n            File \"C:\\Users\\ftn0813\\AppData\\Local\\Temp\\ipykernel_2844\\2642161880.py\", line 14, in __sentence2wordcloud__  *\n                frequencies = self.__freqcount__(tensor)\n            File \"C:\\Users\\ftn0813\\AppData\\Local\\Temp\\ipykernel_2844\\977086180.py\", line 20, in __freqcount__  *\n                words = tensor.numpy().decode().split()\n        \n            AttributeError: 'Tensor' object has no attribute 'numpy'\n        \n        \n        Call arguments received by layer \"sentence2_word_cloud_16878\" \"                 f\"(type Sentence2WordCloud):\n          • inputs=tf.Tensor(shape=(None,), dtype=string)\n    \n    \n    Call arguments received by layer \"cloudy_cnn_5\" \"                 f\"(type CloudyCNN):\n      • x=tf.Tensor(shape=(None,), dtype=string)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ftn0813\\Documents\\GitHub\\ScannerTraining\\Hartzler\\cnn_v2.ipynb Cell 29\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ftn0813/Documents/GitHub/ScannerTraining/Hartzler/cnn_v2.ipynb#X25sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m new_sentences \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mNow we know why some animals eat their own children.\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mgame of thrones season finale showing this sunday night\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mPlease, keep talking. I always yawn when I am interested.\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ftn0813/Documents/GitHub/ScannerTraining/Hartzler/cnn_v2.ipynb#X25sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mround\u001b[39m(model\u001b[39m.\u001b[39;49mpredict(new_sentences)))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filezvwe8e9b.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__predict_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_fileeyr78_pg.py:10\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m      8\u001b[0m do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m      9\u001b[0m retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefinedReturnValue()\n\u001b[1;32m---> 10\u001b[0m x \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39msentence2wordcloud, (ag__\u001b[39m.\u001b[39mld(x),), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     11\u001b[0m x \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mconv1, (ag__\u001b[39m.\u001b[39mld(x),), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     12\u001b[0m x \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mflatten, (ag__\u001b[39m.\u001b[39mld(x),), \u001b[39mNone\u001b[39;00m, fscope)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file6v53uftz.py:24\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     22\u001b[0m new_tensor \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefined(\u001b[39m'\u001b[39m\u001b[39mnew_tensor\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     23\u001b[0m tensor \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefined(\u001b[39m'\u001b[39m\u001b[39mtensor\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 24\u001b[0m ag__\u001b[39m.\u001b[39mfor_stmt(ag__\u001b[39m.\u001b[39mld(inputs), \u001b[39mNone\u001b[39;00m, loop_body, get_state, set_state, (), {\u001b[39m'\u001b[39m\u001b[39miterate_names\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mtensor\u001b[39m\u001b[39m'\u001b[39m})\n\u001b[0;32m     25\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     26\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file6v53uftz.py:20\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call.<locals>.loop_body\u001b[1;34m(itr)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mloop_body\u001b[39m(itr):\n\u001b[0;32m     19\u001b[0m     tensor \u001b[39m=\u001b[39m itr\n\u001b[1;32m---> 20\u001b[0m     new_tensor \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mconvert_to_tensor, (ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(\u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m__sentence2wordcloud__, (ag__\u001b[39m.\u001b[39;49mld(tensor),), \u001b[39mNone\u001b[39;49;00m, fscope),), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     21\u001b[0m     ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(output)\u001b[39m.\u001b[39mappend, (ag__\u001b[39m.\u001b[39mld(new_tensor),), \u001b[39mNone\u001b[39;00m, fscope)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file3gk6jg83.py:10\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf____sentence2wordcloud__\u001b[1;34m(self, tensor)\u001b[0m\n\u001b[0;32m      8\u001b[0m do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m      9\u001b[0m retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefinedReturnValue()\n\u001b[1;32m---> 10\u001b[0m frequencies \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m__freqcount__, (ag__\u001b[39m.\u001b[39mld(tensor),), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     11\u001b[0m cloud \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(WordCloud), (), \u001b[39mdict\u001b[39m(width\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(image_size)[\u001b[39m0\u001b[39m], height\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(image_size)[\u001b[39m1\u001b[39m], stopwords\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m], min_word_length\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, repeat\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, normalize_plurals\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, include_numbers\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, font_path\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(word_cloud_font_path)), fscope)\n\u001b[0;32m     12\u001b[0m image \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(cloud)\u001b[39m.\u001b[39mgenerate_from_frequencies, (ag__\u001b[39m.\u001b[39mld(frequencies),), \u001b[39mNone\u001b[39;00m, fscope)\u001b[39m.\u001b[39mto_array, (), \u001b[39mNone\u001b[39;00m, fscope)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file0l_znj3v.py:10\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf____freqcount__\u001b[1;34m(self, tensor)\u001b[0m\n\u001b[0;32m      8\u001b[0m do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m      9\u001b[0m retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefinedReturnValue()\n\u001b[1;32m---> 10\u001b[0m words \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tensor)\u001b[39m.\u001b[39mnumpy, (), \u001b[39mNone\u001b[39;00m, fscope)\u001b[39m.\u001b[39mdecode, (), \u001b[39mNone\u001b[39;00m, fscope)\u001b[39m.\u001b[39msplit, (), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     11\u001b[0m freq_count \u001b[39m=\u001b[39m [ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(words)\u001b[39m.\u001b[39mcount, (ag__\u001b[39m.\u001b[39mld(k),), \u001b[39mNone\u001b[39;00m, fscope) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m ag__\u001b[39m.\u001b[39mld(words)]\n\u001b[0;32m     12\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[1;31mAttributeError\u001b[0m: in user code:\n\n    File \"C:\\Users\\ftn0813\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 2041, in predict_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\ftn0813\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 2027, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\ftn0813\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 2015, in run_step  **\n        outputs = model.predict_step(data)\n    File \"C:\\Users\\ftn0813\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 1983, in predict_step\n        return self(x, training=False)\n    File \"C:\\Users\\ftn0813\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\ftn0813\\AppData\\Local\\Temp\\__autograph_generated_fileeyr78_pg.py\", line 10, in tf__call\n        x = ag__.converted_call(ag__.ld(self).sentence2wordcloud, (ag__.ld(x),), None, fscope)\n    File \"C:\\Users\\ftn0813\\AppData\\Local\\Temp\\__autograph_generated_file6v53uftz.py\", line 24, in tf__call\n        ag__.for_stmt(ag__.ld(inputs), None, loop_body, get_state, set_state, (), {'iterate_names': 'tensor'})\n    File \"C:\\Users\\ftn0813\\AppData\\Local\\Temp\\__autograph_generated_file6v53uftz.py\", line 20, in loop_body\n        new_tensor = ag__.converted_call(ag__.ld(tf).convert_to_tensor, (ag__.converted_call(ag__.ld(self).__sentence2wordcloud__, (ag__.ld(tensor),), None, fscope),), None, fscope)\n    File \"C:\\Users\\ftn0813\\AppData\\Local\\Temp\\__autograph_generated_file3gk6jg83.py\", line 10, in tf____sentence2wordcloud__\n        frequencies = ag__.converted_call(ag__.ld(self).__freqcount__, (ag__.ld(tensor),), None, fscope)\n    File \"C:\\Users\\ftn0813\\AppData\\Local\\Temp\\__autograph_generated_file0l_znj3v.py\", line 10, in tf____freqcount__\n        words = ag__.converted_call(ag__.converted_call(ag__.converted_call(ag__.ld(tensor).numpy, (), None, fscope).decode, (), None, fscope).split, (), None, fscope)\n\n    AttributeError: Exception encountered when calling layer \"cloudy_cnn_5\" \"                 f\"(type CloudyCNN).\n    \n    in user code:\n    \n        File \"C:\\Users\\ftn0813\\AppData\\Local\\Temp\\ipykernel_2844\\442203258.py\", line 13, in call  *\n            x = self.sentence2wordcloud(x)\n        File \"C:\\Users\\ftn0813\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"C:\\Users\\ftn0813\\AppData\\Local\\Temp\\__autograph_generated_file6v53uftz.py\", line 24, in tf__call\n            ag__.for_stmt(ag__.ld(inputs), None, loop_body, get_state, set_state, (), {'iterate_names': 'tensor'})\n        File \"C:\\Users\\ftn0813\\AppData\\Local\\Temp\\__autograph_generated_file6v53uftz.py\", line 20, in loop_body\n            new_tensor = ag__.converted_call(ag__.ld(tf).convert_to_tensor, (ag__.converted_call(ag__.ld(self).__sentence2wordcloud__, (ag__.ld(tensor),), None, fscope),), None, fscope)\n        File \"C:\\Users\\ftn0813\\AppData\\Local\\Temp\\__autograph_generated_file3gk6jg83.py\", line 10, in tf____sentence2wordcloud__\n            frequencies = ag__.converted_call(ag__.ld(self).__freqcount__, (ag__.ld(tensor),), None, fscope)\n        File \"C:\\Users\\ftn0813\\AppData\\Local\\Temp\\__autograph_generated_file0l_znj3v.py\", line 10, in tf____freqcount__\n            words = ag__.converted_call(ag__.converted_call(ag__.converted_call(ag__.ld(tensor).numpy, (), None, fscope).decode, (), None, fscope).split, (), None, fscope)\n    \n        AttributeError: Exception encountered when calling layer \"sentence2_word_cloud_16878\" \"                 f\"(type Sentence2WordCloud).\n        \n        in user code:\n        \n            File \"C:\\Users\\ftn0813\\AppData\\Local\\Temp\\ipykernel_2844\\977086180.py\", line 9, in call  *\n                new_tensor = tf.convert_to_tensor(self.__sentence2wordcloud__(tensor))\n            File \"C:\\Users\\ftn0813\\AppData\\Local\\Temp\\ipykernel_2844\\2642161880.py\", line 14, in __sentence2wordcloud__  *\n                frequencies = self.__freqcount__(tensor)\n            File \"C:\\Users\\ftn0813\\AppData\\Local\\Temp\\ipykernel_2844\\977086180.py\", line 20, in __freqcount__  *\n                words = tensor.numpy().decode().split()\n        \n            AttributeError: 'Tensor' object has no attribute 'numpy'\n        \n        \n        Call arguments received by layer \"sentence2_word_cloud_16878\" \"                 f\"(type Sentence2WordCloud):\n          • inputs=tf.Tensor(shape=(None,), dtype=string)\n    \n    \n    Call arguments received by layer \"cloudy_cnn_5\" \"                 f\"(type CloudyCNN):\n      • x=tf.Tensor(shape=(None,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "new_sentences = [\"Now we know why some animals eat their own children.\", \"game of thrones season finale showing this sunday night\",\"Please, keep talking. I always yawn when I am interested.\"]\n",
    "new_dataset = tf.data.Dataset.from_tensor_slices(new_sentences)\n",
    "for sentence in new_dataset:\n",
    "    print(round(model.predict(sentence)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./model_saves/cnn/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "deb4792152b8b9767403eeef0a1b0f34b83d442136ccee9184cd7d1131f09aa0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
